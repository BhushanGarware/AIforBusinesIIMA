{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install Libraries & Setup\n",
        "We will use google-genai (the latest Google SDK), pypdf for reading PDFs, and numpy for vector math."
      ],
      "metadata": {
        "id": "s5cmqeDlI9BR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRJ6uTrJBdGU"
      },
      "outputs": [],
      "source": [
        "!pip install google-genai pypdf numpy requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from google import genai\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "import pypdf\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "try:\n",
        "    # Initialize the Google GenAI Client\n",
        "    client = genai.Client(api_key=API_KEY)\n",
        "\n",
        "    # 2. Make a simple test call\n",
        "    print(\"Testing API connection...\")\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=\"Say 'Hello, World! The API is working perfectly.'\"\n",
        "    )\n",
        "\n",
        "    # 3. Print Result\n",
        "    print(\"\\nSUCCESS! üéâ\")\n",
        "    print(f\"Model Response: {response.text}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\nERROR ‚ùå\")\n",
        "    print(f\"Something went wrong: {e}\")"
      ],
      "metadata": {
        "id": "mTKc02RvDHC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Download PDF Files\n",
        "We will download a classic AI paper (\"Attention Is All You Need\") to serve as our knowledge base."
      ],
      "metadata": {
        "id": "T5MwrRLdJDKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_pdf(url, filename):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Downloaded: {filename}\")\n",
        "    else:\n",
        "        print(f\"Failed to download {url}\")\n",
        "\n",
        "# Create a data directory\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# URL for \"Attention Is All You Need\" (The Transformer paper)\n",
        "pdf_url = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
        "pdf_filename = \"data/attention_paper.pdf\"\n",
        "\n",
        "download_pdf(pdf_url, pdf_filename)"
      ],
      "metadata": {
        "id": "bU26XvzMEI2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: PDF Parsing and Chunking\n",
        "RAG requires breaking large documents into smaller, manageable \"chunks\" of text"
      ],
      "metadata": {
        "id": "CA7Dr2VVJJ9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_and_chunk_pdf(filepath, chunk_size=1000, overlap=100):\n",
        "    \"\"\"\n",
        "    Reads a PDF and splits it into overlapping text chunks.\n",
        "    \"\"\"\n",
        "    text_content = \"\"\n",
        "\n",
        "    # 1. Parse PDF to Text\n",
        "    with open(filepath, 'rb') as f:\n",
        "        reader = pypdf.PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                text_content += text + \"\\n\"\n",
        "\n",
        "    # 2. Simple Chunking (Character based for simplicity)\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    text_length = len(text_content)\n",
        "\n",
        "    while start < text_length:\n",
        "        end = start + chunk_size\n",
        "        # If we are not at the end, try to find a space to avoid cutting words\n",
        "        if end < text_length:\n",
        "            # Look for the last space within the chunk to break cleanly\n",
        "            last_space = text_content.rfind(' ', start, end)\n",
        "            if last_space != -1:\n",
        "                end = last_space\n",
        "\n",
        "        chunk = text_content[start:end].strip()\n",
        "        if chunk:\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        # Move forward, subtracting overlap to keep context\n",
        "        start = end - overlap\n",
        "\n",
        "    print(f\"Total Text Length: {len(text_content)} characters\")\n",
        "    print(f\"Total Chunks Created: {len(chunks)}\")\n",
        "    return chunks\n",
        "\n",
        "# Execute\n",
        "chunks = parse_and_chunk_pdf(pdf_filename)\n",
        "\n",
        "# Preview a chunk\n",
        "print(\"\\n--- Sample Chunk ---\")\n",
        "print(chunks[0][:500] + \"...\")"
      ],
      "metadata": {
        "id": "-uiEy-G_B4-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Embedding & Vector Store Creation\n",
        "We transform text chunks into \"vectors\" (lists of numbers). Similar meanings have mathematically similar vectors. We will use a simple Python list as our \"Vector Store\" to show exactly how it works."
      ],
      "metadata": {
        "id": "VFVCUIM0JSQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleVectorStore:\n",
        "    def __init__(self):\n",
        "        self.vectors = []\n",
        "        self.texts = []\n",
        "\n",
        "    def add_documents(self, texts):\n",
        "        \"\"\"\n",
        "        Embeds texts using Google's embedding model and stores them.\n",
        "        \"\"\"\n",
        "        print(\"Generating embeddings... this may take a moment.\")\n",
        "\n",
        "        # Batching is recommended for production, but we'll do simple loops for clarity\n",
        "        # or use the SDK's batch support if available.\n",
        "        # Here we map specific model: text-embedding-004\n",
        "\n",
        "        batch_size = 10 # Process in small batches to avoid hitting limits\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "\n",
        "            # Call Google GenAI Embedding API\n",
        "            response = client.models.embed_content(\n",
        "                model=\"gemini-embedding-001\",\n",
        "                contents=batch_texts\n",
        "            )\n",
        "\n",
        "            # The response contains a list of embeddings\n",
        "            batch_embeddings = [e.values for e in response.embeddings]\n",
        "\n",
        "            self.vectors.extend(batch_embeddings)\n",
        "            self.texts.extend(batch_texts)\n",
        "            print(f\"Processed batch {i} to {i+len(batch_texts)}\")\n",
        "\n",
        "        # Convert to numpy array for fast calculation\n",
        "        self.vectors = np.array(self.vectors)\n",
        "        print(f\"Vector Store Created with {len(self.vectors)} documents.\")\n",
        "\n",
        "    def search(self, query, k=3):\n",
        "        \"\"\"\n",
        "        Retrieves the top-k most relevant chunks for a query.\n",
        "        \"\"\"\n",
        "        # 1. Embed the query\n",
        "        response = client.models.embed_content(\n",
        "            model=\"gemini-embedding-001\",\n",
        "            contents=query\n",
        "        )\n",
        "        query_vector = np.array(response.embeddings[0].values)\n",
        "\n",
        "        # 2. Calculate Cosine Similarity\n",
        "        # (Dot product of normalized vectors)\n",
        "\n",
        "        # Normalize stored vectors\n",
        "        norm_vectors = self.vectors / np.linalg.norm(self.vectors, axis=1, keepdims=True)\n",
        "        # Normalize query vector\n",
        "        norm_query = query_vector / np.linalg.norm(query_vector)\n",
        "\n",
        "        # Dot product\n",
        "        similarities = np.dot(norm_vectors, norm_query)\n",
        "\n",
        "        # 3. Get Top-K Indices\n",
        "        top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_k_indices:\n",
        "            results.append({\n",
        "                \"text\": self.texts[idx],\n",
        "                \"score\": similarities[idx]\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "# Initialize and Populate Vector Store\n",
        "vector_store = SimpleVectorStore()\n",
        "vector_store.add_documents(chunks)"
      ],
      "metadata": {
        "id": "ouKzQ82fB9tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Query Retrieval\n",
        "Now we test the system. We ask a question, and the system finds the specific paragraphs in the PDF that contain the answer."
      ],
      "metadata": {
        "id": "rf37ZOJUJZiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"How does the Transformer model use self-attention?\"\n",
        "\n",
        "print(f\"Query: {user_query}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Retrieve context\n",
        "retrieved_docs = vector_store.search(user_query, k=3)\n",
        "\n",
        "print(\"Retrieved Contexts:\")\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    print(f\"\\n[Result {i+1}] (Score: {doc['score']:.4f})\")\n",
        "    print(doc['text'][:300] + \"...\") # Print first 300 chars of the chunk"
      ],
      "metadata": {
        "id": "zO7fI1bRCGQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Answer Generation (The \"G\" in RAG)\n",
        "Finally, we combine the Query + Retrieved Text and send it to Gemini. This prevents hallucinations by forcing the model to use the source material."
      ],
      "metadata": {
        "id": "-r1y2ygwJkdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_rag_answer(query, retrieved_docs):\n",
        "    # 1. Construct the Prompt with Context\n",
        "    context_text = \"\\n\\n\".join([f\"Context {i+1}: {doc['text']}\" for i, doc in enumerate(retrieved_docs)])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a helpful AI assistant. Use the following context to answer the user's question.\n",
        "    If the answer is not in the context, say \"I cannot find the answer in the provided document.\"\n",
        "\n",
        "    User Question: {query}\n",
        "\n",
        "    ---\n",
        "    {context_text}\n",
        "    ---\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    # 2. Generate Answer using Gemini\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=prompt\n",
        "    )\n",
        "\n",
        "    return response.text\n",
        "\n",
        "# Run the full RAG pipeline\n",
        "print(f\"User Question: {user_query}\\n\")\n",
        "final_answer = generate_rag_answer(user_query, retrieved_docs)\n",
        "\n",
        "print(\"Gemini's RAG Answer:\")\n",
        "print(\"=\" * 50)\n",
        "print(final_answer)\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "Uq8czERsEtBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aboEvXeTE2ym"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}